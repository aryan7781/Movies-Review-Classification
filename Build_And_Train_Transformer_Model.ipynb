{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Build And Train Transformer Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <center> Transformer - Attention Is All You Need !!!  </center>\n",
        "\n",
        "<div class=\"row\">\n",
        "  <div class=\"column\" style = \" float: center; width: 100%; padding: 5px\">\n",
        "    <img src=\"https://www.analyticsinsight.net/wp-content/uploads/2022/05/Know-About-Transformer-Machine-learning-Model-at-a-Glance-1440x564_c.jpg\" alt=\"Positional Embedding in Encoder\" style=\"height:100%\">\n",
        "  </div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "MEHMwtudDG3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download and Import Necessary Packages"
      ],
      "metadata": {
        "id": "0Chu4MQ_CrBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files, drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization, Dense, MaxPooling1D, Conv1D, LSTM, MultiHeadAttention, Flatten\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "YXkRzdMCCpIv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJZXr-dLahio",
        "outputId": "08a0b54f-410f-42d6-c197-c027fb7ab9be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect Google Colab TPU"
      ],
      "metadata": {
        "id": "QBRJCj0uCvv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TPU_NAME']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "apGQs_Y2CwrP",
        "outputId": "41389694-27fc-49ad-f5c2-20c3844362ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'grpc://10.43.6.242:8470'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tpu_address = os.environ['TPU_NAME']\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "print(f\"TPU with address-  {tpu.cluster_spec().as_dict()['worker']} successfully connected\")\n",
        "print(f\"Number Of Accelerators- {strategy.num_replicas_in_sync}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCvYG24sCyyP",
        "outputId": "f00451c2-4ff1-4373-bbbc-d6fca967862c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.43.6.242:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.43.6.242:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU with address-  ['10.43.6.242:8470'] successfully connected\n",
            "Number Of Accelerators- 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "7FFGm0xkIGCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = '/content/drive/My Drive/ML Datasets/Movie Review/100_512'\n",
        "\n",
        "EMBEDDING_MATRIX = np.load(dir_path + '/Embedding_Matrix_100.npy')\n",
        "X_train = np.load(dir_path + '/X_train.npy')\n",
        "Y_train = np.load(dir_path + '/Y_train.npy')\n",
        "\n",
        "X_test = np.load(dir_path + '/X_test.npy')\n",
        "Y_test = np.load(dir_path + '/Y_test.npy')"
      ],
      "metadata": {
        "id": "0HKp4t0PI_jF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train))\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoFQ3A9aJfFL",
        "outputId": "2d17f49c-04e0-4577-830e-a1271f32c098"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(40000, 512) (40000, 1)\n",
            "(10000, 512) (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Constants\n",
        "    -> In the Training loop, we use 100-Dimensional Word Embeddings and keep max length of words to be 512.\n",
        "    -> So we keep only the first 512 non stop words for the classification.\n",
        "    -> Since the task is a simple Classification Task, our model should generalize with this data.\n"
      ],
      "metadata": {
        "id": "BgqO3twxERBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "MAX_SEQ_LEN = 512\n",
        "EMB_DIMS = 100\n",
        "VOCAB_SIZE = EMBEDDING_MATRIX.shape[0]\n",
        "print(\"Batch Size- \", BATCH_SIZE)\n",
        "print(\"Maximum Sequence Length- \", MAX_SEQ_LEN)\n",
        "print(\"vocabulary size-\", VOCAB_SIZE)\n",
        "print(\"Embedding Dimensions- \", EMB_DIMS)"
      ],
      "metadata": {
        "id": "G4vyEsIgIHqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc852d39-6994-4cdb-bf69-232cbba1b62e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size-  128\n",
            "Maximum Sequence Length-  512\n",
            "vocabulary size- 81657\n",
            "Embedding Dimensions-  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_final_dataset(dataset):\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    dataset = dataset.cache()\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "aCRgQvwXhvkU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train[:, :512], Y_train)).shuffle(40000)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_test[:, :512], Y_test)).shuffle(10000)\n",
        "train_dataset = get_final_dataset(train_dataset)\n",
        "val_dataset = get_final_dataset(val_dataset)\n"
      ],
      "metadata": {
        "id": "zphTzpvVI8qw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masks"
      ],
      "metadata": {
        "id": "BEQqKq9LgTNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    ## Adding required dimensions to the padding\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]        # (Batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "# Working Example\n",
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "create_padding_mask(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBAWT109gouF",
        "outputId": "e550686b-eb33-4fff-f91a-c4fbd01afc28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center style = 'font-family:\"courier\"'> **Transformer Encoder** </center>\n",
        "<center>\n",
        "    <img src = \"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\"> </img>\n",
        "    </cener>"
      ],
      "metadata": {
        "id": "H-R6EoknA20Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align = \"justify\" style = 'font-family = \"courier\"'>Simple RNN Networks like GRUs or LSTMs do not work very well for long sentences. Simply put forward, it is difficult to keep all the information learnt in a single vector (the activation vector, **a** of the model for long sentences. After certain timesteps, the previous information tends to be lost, while current ones being captured. To address this issue, **Attention Mechanism** was introduced which took the NLP research with a storm. Everything was about **Attention** then.\n",
        "In 2017 a research paper was published, namely **Attention Is All You Need**. This was when Transformers came into existance. Transformers tried to combine the characteristics of **Convolutional Neural Networks and Attention Mechanism**. \n",
        "One main difference  between other architectures and transformer is that the **input sequence can be passed parallelly** so that GPU can be used effectively and the speed of training can also be increased. It is also based on the multi-headed attention layer, so it easily overcomes the vanishing gradient issue. Transformers made **Vectorization** possible for working on Text Data and thus increasing the speed of of training.</p>"
      ],
      "metadata": {
        "id": "ks0aK4rLBX1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Embeddings\n",
        "<p align = \"justify\">In the earlier approach, for sequential models, inputs were fed in sequence, so the effect of positions were automatically captured by the learning algorithm. But, in **Transformer Architecture**, inputs were all fed at once, so making it difficult for the architecture to guess the impact of position of a word in the sentence/paragraph. Thus, **Positional Encodings** were brought in.\n",
        "The Inputs after being converted into Embedded Matrices are added with the positional embedding matrix for the architecture to have knowledge about the position of the words.\n",
        "\n",
        "    \n",
        "<div class=\"row\">\n",
        "  <div class=\"column\" style = \" float: left; width: 30%; padding: 5px\">\n",
        "    <img src=\"https://vaclavkosar.com/images/transformer-positional-embeddings.png\" alt=\"Positional Embedding in Encoder\">\n",
        "  </div>\n",
        "  <div class=\"column\" style = \"float: left; width: 50%; padding: 5px\">\n",
        "    <img src=\"https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-10.jpg\" alt=\"Embeddings working\">\n",
        "  </div>\n",
        "</div>"
      ],
      "metadata": {
        "id": "vq8T8hC2Baif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positional_embedding(num_positions:int, dimensions:int):\n",
        "    \"\"\"\n",
        "    num_positions: Length Of Sequences in the dataset after padding\n",
        "    dimensions: Number of dimensions used to represent each word in embedding matrix\n",
        "    \"\"\"\n",
        "    # Create a column vector for positions\n",
        "    pos_vec = np.arange(num_positions)[:, np.newaxis]\n",
        "    \n",
        "    # Create a row vector for dimensions\n",
        "    dims_vec = np.arange(dimensions)[np.newaxis, :]\n",
        "    \n",
        "    i = dims_vec // 2\n",
        "    angles = pos_vec * 1.0 / (pow(10_000, 2 * i / dimensions))\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    angles[:, 0::1] = np.cos(angles[:, 0::1])\n",
        "    pos_encoding = angles[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)"
      ],
      "metadata": {
        "id": "5kDjoq6lBhBA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_positional_embedding(10, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmIXkSZFBvG1",
        "outputId": "cb72d7c5-76ab-4a42-9622-8110417a8b4b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10, 4), dtype=float32, numpy=\n",
              "array([[[ 1.        ,  1.        ,  1.        ,  1.        ],\n",
              "        [ 0.66636676,  0.5403023 ,  0.99995   ,  0.99995   ],\n",
              "        [ 0.6143003 , -0.41614684,  0.9998    ,  0.9998    ],\n",
              "        [ 0.9900591 , -0.9899925 ,  0.99955016,  0.99955004],\n",
              "        [ 0.7270351 , -0.6536436 ,  0.9992005 ,  0.9992001 ],\n",
              "        [ 0.5744009 ,  0.2836622 ,  0.9987513 ,  0.99875027],\n",
              "        [ 0.9612168 ,  0.96017027,  0.9982027 ,  0.99820054],\n",
              "        [ 0.7918362 ,  0.75390226,  0.997555  ,  0.997551  ],\n",
              "        [ 0.5492263 , -0.14550003,  0.9968085 ,  0.99680173],\n",
              "        [ 0.9162743 , -0.91113025,  0.99596363,  0.9959527 ]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer"
      ],
      "metadata": {
        "id": "0s6rd5DXBx2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dims, num_heads,\n",
        "                  fully_connected_dim, dropout_rate = 0.1, \n",
        "                  layernorm_eps = 1e-6):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        \n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads,\n",
        "                                                      key_dim = embedding_dims,\n",
        "                                                      dropout = dropout_rate)\n",
        "        \n",
        "        self.dense1 = tf.keras.layers.Dense(fully_connected_dim, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(embedding_dims)\n",
        "        \n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = layernorm_eps)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = layernorm_eps)\n",
        "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
        "        \n",
        "    def call(self, x, padding_mask):\n",
        "        attn_out = self.mha(x, x, x, padding_mask)\n",
        "        attn_out = self.layernorm1(x + attn_out)\n",
        "        ffn_out = self.dense1(attn_out)\n",
        "        ffn_out = self.dense2(attn_out)\n",
        "        ffn_out = self.layernorm2(attn_out + ffn_out)\n",
        "        return attn_out"
      ],
      "metadata": {
        "id": "U3PmadrwBwsl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "s7Ng9G6fB1ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_vocab_size, embedding_dims, num_heads,\n",
        "                 fully_connected_dim, max_position_encoding, pretrained_embedding_matrix = None,\n",
        "                 dropout_rate = 0.1, layernorm_eps = 1e-6):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.embedding_dims = embedding_dims\n",
        "        \n",
        "        \n",
        "        self.embeddings = tf.keras.layers.Embedding(input_vocab_size, self.embedding_dims)\n",
        "        if pretrained_embedding_matrix is not None:\n",
        "            self.embeddings.embeddings_initializer = tf.keras.initializers.constant(pretrained_embedding_matrix)\n",
        "            self.embeddings.trainable = False\n",
        "        self.positional_encoding = get_positional_embedding(max_position_encoding, self.embedding_dims)\n",
        "        self.encoding_layer = EncoderLayer(embedding_dims, num_heads,\n",
        "                  fully_connected_dim, dropout_rate = dropout_rate, \n",
        "                  layernorm_eps = layernorm_eps)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        \n",
        "        x = self.embeddings(x)\n",
        "        \n",
        "        ## Scaling the embeddings\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dims, tf.float32))\n",
        "        \n",
        "        ## Adding the Positional Encoding to embeddings\n",
        "        x += self.positional_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.encoding_layer(x, padding_mask)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "t9dTQW6rB3au"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_encoder = Encoder(VOCAB_SIZE, EMB_DIMS, 2,\n",
        "                 512, 512, pretrained_embedding_matrix = EMBEDDING_MATRIX)\n",
        "temp_input = tf.random.uniform((128, 512), dtype=tf.int64, minval=0, maxval=200)\n",
        "padding_mask = create_padding_mask(temp_input)\n",
        "sample_encoder_output = sample_encoder(temp_input, padding_mask=padding_mask)\n",
        "\n",
        "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HULwt3MHiJBa",
        "outputId": "3f1f4fe6-c26d-4b23-944b-fc782977e042"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 512, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Learning Model"
      ],
      "metadata": {
        "id": "X0ud8mQGB6Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Encoder_Model(tf.keras.Model):\n",
        "    def __init__(self, input_shape, input_vocab_size, embedding_dims, num_heads, fully_connected_dim, embedding_matrix = None):\n",
        "        super(Transformer_Encoder_Model, self).__init__()\n",
        "        self.encoder = Encoder(input_vocab_size = input_vocab_size,\n",
        "                      embedding_dims = embedding_dims,\n",
        "                      num_heads = num_heads,\n",
        "                     fully_connected_dim = fully_connected_dim,\n",
        "                      max_position_encoding = input_shape, pretrained_embedding_matrix = embedding_matrix,\n",
        "                     dropout_rate = 0.1, layernorm_eps = 1e-6\n",
        "        )\n",
        "        self.batchnorm1 = tf.keras.layers.BatchNormalization(axis = 1)\n",
        "        self.conv1 = Conv1D(64, kernel_size = 8, strides = 4)\n",
        "        self.maxpool1 = MaxPooling1D()\n",
        "        self.conv2 = Conv1D(128, kernel_size = 8, strides = 4)\n",
        "        self.maxpool2 = MaxPooling1D()\n",
        "        self.flatten = Flatten()\n",
        "        self.dense1 = Dense(512, activation = 'relu')\n",
        "        self.batchnorm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = Dense(32, activation = 'relu')\n",
        "        self.dense3 = Dense(1, activation = 'sigmoid')\n",
        "\n",
        "    def call(self, x):\n",
        "        padding_mask = create_padding_mask(x)\n",
        "        encoder_out = self.encoder(x, padding_mask)\n",
        "        x  = self.batchnorm1(encoder_out)\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "SWGjI6qolmTw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def Mymodel(input_shape, input_vocab_size, embedding_dims, embedding_matrix = None):\n",
        "#     input_x = tf.keras.layers.Input(shape = input_shape)\n",
        "#     encoder_out = Encoder(input_vocab_size = input_vocab_size,\n",
        "#                       embedding_dims = embedding_dims,\n",
        "#                       num_heads = 3,\n",
        "#                      fully_connected_dim = 128,\n",
        "#                       max_position_encoding = input_shape, pretrained_embedding_matrix = embedding_matrix,\n",
        "#                      dropout_rate = 0.1, layernorm_eps = 1e-6\n",
        "#         )(input_x)\n",
        "\n",
        "#     x = tf.keras.layers.BatchNormalization(axis = 1)(encoder_out)\n",
        "#     x = Conv1D(64, kernel_size = 8, strides = 4)(x)\n",
        "#     x = MaxPooling1D()(x)\n",
        "#     x = Conv1D(128, kernel_size = 8, strides = 4)(x)\n",
        "#     x = MaxPooling1D()(x)\n",
        "#     x = Flatten()(x)\n",
        "#     x = Dense(512, activation = 'relu')(x)\n",
        "#     x = tf.keras.layers.BatchNormalization()(x)\n",
        "#     x = Dense(32, activation = 'relu')(x)\n",
        "#     x = Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "#     model = tf.keras.Model(inputs = input_x, outputs = x)\n",
        "#     return model"
      ],
      "metadata": {
        "id": "fEEbHECLB981"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Training Loop"
      ],
      "metadata": {
        "id": "TRv7moS_CShG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    model = Transformer_Encoder_Model(MAX_SEQ_LEN, VOCAB_SIZE,\n",
        "                                  EMB_DIMS, 2,\n",
        "                                  512, embedding_matrix = EMBEDDING_MATRIX)\n",
        "\n",
        "    loss_object = tf.keras.losses.BinaryCrossentropy(reduction = tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "    def compute_loss(labels, predictions):\n",
        "        per_example_loss = loss_object(labels, predictions)\n",
        "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync)\n",
        "\n",
        "    test_loss = tf.keras.metrics.Mean(name = \"test_loss\")\n",
        "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name = \"train_accuracy\")\n",
        "    test_accuracy = tf.keras.metrics.BinaryAccuracy(name = \"test_accuracy\")\n",
        "\n",
        "    lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate = 0.0005,\n",
        "        decay_steps =500,\n",
        "        decay_rate = 0.9\n",
        "    )\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = lr_scheduler)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_training_step(datasets_inputs):\n",
        "        per_replica_losses = strategy.run(train_steps, args = (datasets_inputs, ))\n",
        "        print(per_replica_losses)\n",
        "        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis = None)\n",
        "    \n",
        "    @tf.function\n",
        "    def distributed_test_step(datasets_inputs):\n",
        "        strategy.run(test_steps, args = (datasets_inputs, ))\n",
        "    \n",
        "    \n",
        "    def train_steps(inputs):\n",
        "        text_seq, labels = inputs\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(text_seq)\n",
        "            loss = compute_loss(labels, predictions)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        train_accuracy.update_state(labels, predictions)\n",
        "        return loss\n",
        "\n",
        "    def test_steps(inputs):\n",
        "        text_seq, labels = inputs\n",
        "        predictions = model(text_seq)\n",
        "        loss = loss_object(labels, predictions)\n",
        "\n",
        "        test_loss.update_state(loss)\n",
        "        test_accuracy.update_state(labels, predictions)"
      ],
      "metadata": {
        "id": "3UMM7S8lCVSF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 20\n",
        "with strategy.scope():\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_start = datetime.now()\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Training Loop\n",
        "        for x in train_dataset: \n",
        "            total_loss += distributed_training_step(x)\n",
        "            num_batches += 1\n",
        "\n",
        "        train_loss = total_loss / num_batches\n",
        "\n",
        "        # Testing Loop\n",
        "        for x in val_dataset:\n",
        "            distributed_test_step(x)\n",
        "\n",
        "        epoch_end = datetime.now()\n",
        "        \n",
        "        template = (\"Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}, Test Loss: {:.2f}, Test Accuracy: {:.2f}, \\t Elapsed Time: {}\")\n",
        "\n",
        "        print(template.format(\n",
        "            epoch + 1,\n",
        "            train_loss,\n",
        "            train_accuracy.result() * 100,\n",
        "            test_loss.result() / strategy.num_replicas_in_sync,\n",
        "            test_accuracy.result() * 100,\n",
        "            (epoch_end - epoch_start).seconds\n",
        "        ))\n",
        "\n",
        "        test_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "\n",
        "        test_accuracy.reset_states()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHw9L_LNLANF",
        "outputId": "ed145a40-7b0e-4e25-cbe0-afb877cad284"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['transformer__encoder__model/encoder_1/encoder_layer_1/dense_2/kernel:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_2/bias:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_3/kernel:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_3/bias:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/layer_normalization_3/gamma:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/layer_normalization_3/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['transformer__encoder__model/encoder_1/encoder_layer_1/dense_2/kernel:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_2/bias:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_3/kernel:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_3/bias:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/layer_normalization_3/gamma:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/layer_normalization_3/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerReplica:{\n",
            "  0: Tensor(\"output_0_shard_0:0\", shape=(), dtype=float32),\n",
            "  1: Tensor(\"output_0_shard_1:0\", shape=(), dtype=float32),\n",
            "  2: Tensor(\"output_0_shard_2:0\", shape=(), dtype=float32),\n",
            "  3: Tensor(\"output_0_shard_3:0\", shape=(), dtype=float32),\n",
            "  4: Tensor(\"output_0_shard_4:0\", shape=(), dtype=float32),\n",
            "  5: Tensor(\"output_0_shard_5:0\", shape=(), dtype=float32),\n",
            "  6: Tensor(\"output_0_shard_6:0\", shape=(), dtype=float32),\n",
            "  7: Tensor(\"output_0_shard_7:0\", shape=(), dtype=float32)\n",
            "}\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['transformer__encoder__model/encoder_1/encoder_layer_1/dense_2/kernel:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_2/bias:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_3/kernel:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_3/bias:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/layer_normalization_3/gamma:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/layer_normalization_3/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['transformer__encoder__model/encoder_1/encoder_layer_1/dense_2/kernel:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_2/bias:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_3/kernel:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/dense_3/bias:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/layer_normalization_3/gamma:0', 'transformer__encoder__model/encoder_1/encoder_layer_1/layer_normalization_3/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PerReplica:{\n",
            "  0: Tensor(\"output_0_shard_0:0\", shape=(), dtype=float32),\n",
            "  1: Tensor(\"output_0_shard_1:0\", shape=(), dtype=float32),\n",
            "  2: Tensor(\"output_0_shard_2:0\", shape=(), dtype=float32),\n",
            "  3: Tensor(\"output_0_shard_3:0\", shape=(), dtype=float32),\n",
            "  4: Tensor(\"output_0_shard_4:0\", shape=(), dtype=float32),\n",
            "  5: Tensor(\"output_0_shard_5:0\", shape=(), dtype=float32),\n",
            "  6: Tensor(\"output_0_shard_6:0\", shape=(), dtype=float32),\n",
            "  7: Tensor(\"output_0_shard_7:0\", shape=(), dtype=float32)\n",
            "}\n",
            "Epoch 1, Loss: 0.70, Accuracy: 52.95, Test Loss: 0.08, Test Accuracy: 66.09, \t Elapsed Time: 20\n",
            "Epoch 2, Loss: 0.50, Accuracy: 75.68, Test Loss: 0.06, Test Accuracy: 78.07, \t Elapsed Time: 9\n",
            "Epoch 3, Loss: 0.42, Accuracy: 80.98, Test Loss: 0.06, Test Accuracy: 79.07, \t Elapsed Time: 9\n",
            "Epoch 4, Loss: 0.37, Accuracy: 83.71, Test Loss: 0.05, Test Accuracy: 80.57, \t Elapsed Time: 9\n",
            "Epoch 5, Loss: 0.31, Accuracy: 86.78, Test Loss: 0.06, Test Accuracy: 80.41, \t Elapsed Time: 9\n",
            "Epoch 6, Loss: 0.26, Accuracy: 89.46, Test Loss: 0.08, Test Accuracy: 75.00, \t Elapsed Time: 9\n",
            "Epoch 7, Loss: 0.22, Accuracy: 91.26, Test Loss: 0.07, Test Accuracy: 78.12, \t Elapsed Time: 9\n",
            "Epoch 8, Loss: 0.16, Accuracy: 94.31, Test Loss: 0.08, Test Accuracy: 78.44, \t Elapsed Time: 9\n",
            "Epoch 9, Loss: 0.12, Accuracy: 95.63, Test Loss: 0.10, Test Accuracy: 78.05, \t Elapsed Time: 9\n",
            "Epoch 10, Loss: 0.11, Accuracy: 95.95, Test Loss: 0.10, Test Accuracy: 77.51, \t Elapsed Time: 9\n",
            "Epoch 11, Loss: 0.09, Accuracy: 96.83, Test Loss: 0.11, Test Accuracy: 78.15, \t Elapsed Time: 9\n",
            "Epoch 12, Loss: 0.05, Accuracy: 98.55, Test Loss: 0.12, Test Accuracy: 77.96, \t Elapsed Time: 9\n",
            "Epoch 13, Loss: 0.04, Accuracy: 98.90, Test Loss: 0.15, Test Accuracy: 76.68, \t Elapsed Time: 9\n",
            "Epoch 14, Loss: 0.03, Accuracy: 99.23, Test Loss: 0.17, Test Accuracy: 76.34, \t Elapsed Time: 9\n",
            "Epoch 15, Loss: 0.02, Accuracy: 99.22, Test Loss: 0.17, Test Accuracy: 77.12, \t Elapsed Time: 9\n",
            "Epoch 16, Loss: 0.02, Accuracy: 99.26, Test Loss: 0.18, Test Accuracy: 77.33, \t Elapsed Time: 9\n",
            "Epoch 17, Loss: 0.01, Accuracy: 99.61, Test Loss: 0.18, Test Accuracy: 77.71, \t Elapsed Time: 9\n",
            "Epoch 18, Loss: 0.01, Accuracy: 99.80, Test Loss: 0.18, Test Accuracy: 77.35, \t Elapsed Time: 9\n",
            "Epoch 19, Loss: 0.00, Accuracy: 99.91, Test Loss: 0.19, Test Accuracy: 77.73, \t Elapsed Time: 9\n",
            "Epoch 20, Loss: 0.00, Accuracy: 99.97, Test Loss: 0.20, Test Accuracy: 77.90, \t Elapsed Time: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "    -> The Model is heavily overfit to the training data,\n",
        "    -> We shall try a simpler model, or apply regularizations to reduce overfitting."
      ],
      "metadata": {
        "id": "xFxL7nnisH_P"
      }
    }
  ]
}